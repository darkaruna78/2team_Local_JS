{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸï¸ Motorcycle Night Ride - ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ (ë”¥ëŸ¬ë‹ ì„¸ê·¸ë©˜í…Œì´ì…˜)\n",
    "\n",
    "## í”„ë¡œì íŠ¸ ê°œìš”\n",
    "- **ë°ì´í„°ì…‹**: Acme AI Open Dataset - Motorcycle Night Ride (ì•¼ê°„ ì˜¤í† ë°”ì´ ì£¼í–‰, ~200í”„ë ˆì„)\n",
    "- **ëª©í‘œ**: ì„¸ë§Œí‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ë¡œ ë„ë¡œ ì¥ë©´ì„ ë¶„ì„í•˜ì—¬ **í”„ë ˆì„ë³„ ì•ˆì „ ì ìˆ˜** ì‚°ì¶œ\n",
    "- **6ê°œ í´ë˜ìŠ¤**: Undrivable, Road, Lanemark, My bike, Rider, Movable\n",
    "\n",
    "### ì‚¬ìš© ëª¨ë¸\n",
    "| ëª¨ë¸ | íŠ¹ì§• |\n",
    "|------|------|\n",
    "| **DeepLabV3+** | Atrous convolution + Encoder-Decoder, torchvision ì œê³µ |\n",
    "| **SegFormer** | Transformer ê¸°ë°˜, HuggingFace pretrained |\n",
    "| **BiSeNetV2** | ê²½ëŸ‰ ì‹¤ì‹œê°„ ëª¨ë¸, Detail + Semantic ì´ì¤‘ ê²½ë¡œ |\n",
    "\n",
    "### ì¶”ê°€ ë¶„ì„\n",
    "- **GradCAM**: ê° í´ë˜ìŠ¤ ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹œ ì´ë¯¸ì§€ ì˜ì—­ ì‹œê°í™”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "# ============================================================\n",
    "import subprocess, sys\n",
    "\n",
    "def install(pkg):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', pkg])\n",
    "\n",
    "packages = [\n",
    "    'torch', 'torchvision', 'torchaudio',\n",
    "    'transformers',\n",
    "    'pycocotools',\n",
    "    'pytorch-grad-cam',\n",
    "    'albumentations',\n",
    "    'numpy', 'pandas', 'matplotlib', 'seaborn', 'Pillow', 'scikit-learn'\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        install(pkg)\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ {pkg} ì„¤ì¹˜ ì‹¤íŒ¨: {e}')\n",
    "\n",
    "print('âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ & í™˜ê²½ ì„¤ì •\n",
    "# ============================================================\n",
    "import os, json, warnings, gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ (macOS)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ')\n",
    "print(f'ğŸ–¥ï¸  Device: {device}')\n",
    "print(f'ğŸ”¥ PyTorch: {torch.__version__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. ë°ì´í„° EDA\n",
    "\n",
    "COCO í¬ë§· ì–´ë…¸í…Œì´ì…˜ì„ ë¡œë”©í•˜ê³ , ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ë¥¼ ìƒì„±í•˜ì—¬ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1-1. ë°ì´í„° ê²½ë¡œ ì„¤ì • & COCO JSON ë¡œë”©\n",
    "# ============================================================\n",
    "DATA_DIR = Path('..') / 'www.acmeai.tech ODataset 1 - Motorcycle Night Ride Dataset'\n",
    "IMAGE_DIR = DATA_DIR / 'images'\n",
    "JSON_PATH = DATA_DIR / 'COCO_motorcycle (pixel).json'\n",
    "\n",
    "# íŒŒì¼ ë¶„ë¥˜\n",
    "all_files = sorted(os.listdir(IMAGE_DIR))\n",
    "original_files = [f for f in all_files if not f.endswith('___fuse.png') and not f.endswith('___save.png')]\n",
    "mask_files_png = [f for f in all_files if f.endswith('___save.png')]\n",
    "fuse_files = [f for f in all_files if f.endswith('___fuse.png')]\n",
    "\n",
    "print(f'ğŸ“Š ì›ë³¸ ì´ë¯¸ì§€: {len(original_files)}ì¥')\n",
    "print(f'ğŸ“Š ë§ˆìŠ¤í¬ ì´ë¯¸ì§€: {len(mask_files_png)}ì¥')\n",
    "print(f'ğŸ“Š Fuse ì´ë¯¸ì§€: {len(fuse_files)}ì¥')\n",
    "\n",
    "# COCO API ë¡œë”©\n",
    "print('\\nâ³ COCO JSON ë¡œë”© ì¤‘ (200MB+, ì‹œê°„ ì†Œìš”)...')\n",
    "coco = COCO(str(JSON_PATH))\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ ì •ë³´\n",
    "cat_ids = coco.getCatIds()\n",
    "cats = coco.loadCats(cat_ids)\n",
    "cat_id_to_name = {c['id']: c['name'] for c in cats}\n",
    "img_ids = coco.getImgIds()\n",
    "\n",
    "print(f'\\nâœ… ë¡œë”© ì™„ë£Œ!')\n",
    "print(f'ğŸ–¼ï¸  ì´ë¯¸ì§€ ìˆ˜: {len(img_ids)}')\n",
    "print(f'ğŸ“ ì–´ë…¸í…Œì´ì…˜ ìˆ˜: {len(coco.getAnnIds())}')\n",
    "print(f'\\nğŸ·ï¸  ì¹´í…Œê³ ë¦¬ ({len(cats)}ê°œ):')\n",
    "for c in cats:\n",
    "    print(f'   ID {c[\"id\"]}: {c[\"name\"]}')\n",
    "\n",
    "# í´ë˜ìŠ¤ ID ì¬ë§¤í•‘ (0=Background, 1~N=í´ë˜ìŠ¤)\n",
    "sorted_cat_ids = sorted(cat_id_to_name.keys())\n",
    "cat_remap = {0: 0}\n",
    "for new_id, orig_id in enumerate(sorted_cat_ids, start=1):\n",
    "    cat_remap[orig_id] = new_id\n",
    "\n",
    "num_classes = len(cat_remap)\n",
    "id_to_name = {0: 'Background'}\n",
    "for orig_id, new_id in cat_remap.items():\n",
    "    if orig_id != 0:\n",
    "        id_to_name[new_id] = cat_id_to_name[orig_id]\n",
    "class_names = [id_to_name[i] for i in range(num_classes)]\n",
    "\n",
    "print(f'\\nğŸ“– í´ë˜ìŠ¤ ë§¤í•‘ (ì´ {num_classes}ê°œ):')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'   {i}: {name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1-2. ë§ˆìŠ¤í¬ ìƒì„± í•¨ìˆ˜ + ìƒ˜í”Œ ì‹œê°í™” + í´ë˜ìŠ¤ ë¶„í¬\n",
    "# ============================================================\n",
    "def create_class_mask(coco_api, img_id, remap):\n",
    "    \"\"\"COCO ì–´ë…¸í…Œì´ì…˜ì—ì„œ í´ë˜ìŠ¤ ID ë§ˆìŠ¤í¬ ìƒì„±\"\"\"\n",
    "    img_info = coco_api.loadImgs(img_id)[0]\n",
    "    h, w = img_info['height'], img_info['width']\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    ann_ids = coco_api.getAnnIds(imgIds=img_id)\n",
    "    anns = coco_api.loadAnns(ann_ids)\n",
    "    for ann in anns:\n",
    "        try:\n",
    "            binary_mask = coco_api.annToMask(ann)\n",
    "            new_id = remap.get(ann['category_id'], 0)\n",
    "            mask[binary_mask > 0] = new_id\n",
    "        except:\n",
    "            pass\n",
    "    return mask\n",
    "\n",
    "# ìƒ˜í”Œ ì‹œê°í™” (3ì¥)\n",
    "sample_ids = [img_ids[0], img_ids[len(img_ids)//3], img_ids[2*len(img_ids)//3]]\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
    "\n",
    "for i, img_id in enumerate(sample_ids):\n",
    "    info = coco.loadImgs(img_id)[0]\n",
    "    fname = info['file_name']\n",
    "    img = Image.open(IMAGE_DIR / fname)\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(f'ì›ë³¸: {fname}', fontsize=9)\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    mask = create_class_mask(coco, img_id, cat_remap)\n",
    "    axes[i, 1].imshow(mask, cmap='tab10', vmin=0, vmax=num_classes-1)\n",
    "    axes[i, 1].set_title('Class Mask', fontsize=9)\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    fuse_path = IMAGE_DIR / (fname + '___fuse.png')\n",
    "    if fuse_path.exists():\n",
    "        axes[i, 2].imshow(Image.open(fuse_path))\n",
    "    axes[i, 2].set_title('Fuse (ì˜¤ë²„ë ˆì´)', fontsize=9)\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.suptitle('ìƒ˜í”Œ: ì›ë³¸ / Class Mask / Fuse', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„\n",
    "print('â³ ì „ì²´ ì´ë¯¸ì§€ í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ì¤‘...')\n",
    "class_pixel_counts = Counter()\n",
    "total_pixels = 0\n",
    "for img_id in img_ids:\n",
    "    mask = create_class_mask(coco, img_id, cat_remap)\n",
    "    unique, counts = np.unique(mask, return_counts=True)\n",
    "    for u, c in zip(unique, counts):\n",
    "        class_pixel_counts[u] += c\n",
    "    total_pixels += mask.size\n",
    "\n",
    "class_ratios = {class_names[k]: v / total_pixels for k, v in class_pixel_counts.items() if k < num_classes}\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names = list(class_ratios.keys())\n",
    "ratios = [class_ratios[n] for n in names]\n",
    "ax.barh(names, ratios, color=sns.color_palette('Set2', len(names)))\n",
    "ax.set_xlabel('ë©´ì  ë¹„ìœ¨')\n",
    "ax.set_title('ì „ì²´ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë³„ ë©´ì  ë¹„ìœ¨')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for n, r in sorted(class_ratios.items(), key=lambda x: -x[1]):\n",
    "    print(f'  {n:>15s}: {r*100:.2f}%')\n",
    "print(f'\\nğŸ“ ì´ë¯¸ì§€ í•´ìƒë„: {img.size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "PyTorch Dataset/DataLoaderë¥¼ êµ¬ì„±í•˜ê³ , Albumentations ê¸°ë°˜ ë°ì´í„° ì¦ê°•ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "- COCO ì–´ë…¸í…Œì´ì…˜ â†’ í´ë˜ìŠ¤ ID ë§ˆìŠ¤í¬ (on-the-fly)\n",
    "- ì´ë¯¸ì§€ 512x512 ë¦¬ì‚¬ì´ì¦ˆ, ImageNet ì •ê·œí™”\n",
    "- Train 80% / Val 20% Stratified Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2-1. Dataset í´ë˜ìŠ¤ + Transforms + DataLoader\n",
    "# ============================================================\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 15\n",
    "LR = 1e-4\n",
    "\n",
    "class MotorcycleSegDataset(Dataset):\n",
    "    def __init__(self, coco_api, img_ids, image_dir, cat_remap, transform=None):\n",
    "        self.coco = coco_api\n",
    "        self.img_ids = img_ids\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.cat_remap = cat_remap\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        info = self.coco.loadImgs(img_id)[0]\n",
    "        img = np.array(Image.open(self.image_dir / info['file_name']).convert('RGB'))\n",
    "\n",
    "        # ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        h, w = info['height'], info['width']\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        for ann in self.coco.loadAnns(ann_ids):\n",
    "            try:\n",
    "                bm = self.coco.annToMask(ann)\n",
    "                mask[bm > 0] = self.cat_remap.get(ann['category_id'], 0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if self.transform:\n",
    "            t = self.transform(image=img, mask=mask)\n",
    "            img, mask = t['image'], t['mask']\n",
    "\n",
    "        return img.float(), mask.long()\n",
    "\n",
    "# Albumentations ë³€í™˜\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Train / Val ë¶„í• \n",
    "train_ids, val_ids = train_test_split(img_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MotorcycleSegDataset(coco, train_ids, IMAGE_DIR, cat_remap, train_transform)\n",
    "val_dataset = MotorcycleSegDataset(coco, val_ids, IMAGE_DIR, cat_remap, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f'ğŸ“Š Train: {len(train_dataset)}ì¥, Val: {len(val_dataset)}ì¥')\n",
    "print(f'ğŸ“Š Batch size: {BATCH_SIZE}, Image size: {IMG_SIZE}x{IMG_SIZE}')\n",
    "print(f'ğŸ“Š í´ë˜ìŠ¤ ìˆ˜: {num_classes} (Background í¬í•¨)')\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "sample_img, sample_mask = train_dataset[0]\n",
    "print(f'ğŸ“ Image shape: {sample_img.shape}, Mask shape: {sample_mask.shape}')\n",
    "print(f'ğŸ“ Mask unique values: {torch.unique(sample_mask).tolist()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡ \n",
    "\n",
    "3ê°œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ fine-tuningí•˜ê³ , mIoU / Pixel Accuracyë¡œ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "- **DeepLabV3+**: ResNet-50 backbone, Atrous Spatial Pyramid Pooling\n",
    "- **SegFormer-B0**: Mix Transformer encoder, lightweight MLP decoder\n",
    "- **BiSeNetV2**: Detail Branch + Semantic Branch ì´ì¤‘ ê²½ë¡œ ì‹¤ì‹œê°„ ëª¨ë¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-0. í•™ìŠµ/í‰ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "# ============================================================\n",
    "def train_one_epoch(model, loader, criterion, optimizer, dev, model_type='standard'):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, masks in loader:\n",
    "        images, masks = images.to(dev), masks.to(dev)\n",
    "        if model_type == 'segformer':\n",
    "            out = model(pixel_values=images, labels=masks)\n",
    "            loss = out.loss\n",
    "        else:\n",
    "            out = model(images)\n",
    "            logits = out['out'] if isinstance(out, dict) else out\n",
    "            loss = criterion(logits, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def compute_miou(model, loader, n_classes, dev, model_type='standard'):\n",
    "    model.eval()\n",
    "    confusion = np.zeros((n_classes, n_classes), dtype=np.int64)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(dev)\n",
    "            if model_type == 'segformer':\n",
    "                logits = model(pixel_values=images).logits\n",
    "                logits = F.interpolate(logits, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                out = model(images)\n",
    "                logits = out['out'] if isinstance(out, dict) else out\n",
    "            preds = logits.argmax(1).cpu().numpy()\n",
    "            gt = masks.numpy()\n",
    "            for p, g in zip(preds, gt):\n",
    "                valid = g < n_classes\n",
    "                confusion += np.bincount(\n",
    "                    n_classes * g[valid].astype(int) + p[valid].astype(int),\n",
    "                    minlength=n_classes**2\n",
    "                ).reshape(n_classes, n_classes)\n",
    "    iou = np.diag(confusion) / (confusion.sum(0) + confusion.sum(1) - np.diag(confusion) + 1e-10)\n",
    "    pixel_acc = np.diag(confusion).sum() / (confusion.sum() + 1e-10)\n",
    "    return np.nanmean(iou), iou, pixel_acc\n",
    "\n",
    "def train_model(model, name, loader_tr, loader_val, n_cls, dev,\n",
    "                epochs=NUM_EPOCHS, lr=LR, model_type='standard'):\n",
    "    \"\"\"ë²”ìš© í•™ìŠµ ë£¨í”„\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.to(dev)\n",
    "    history = []\n",
    "    print(f'\\nğŸ”¬ {name} í•™ìŠµ ì‹œì‘ ({epochs} epochs, lr={lr})')\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_one_epoch(model, loader_tr, criterion, optimizer, dev, model_type)\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "            miou, iou_cls, pix_acc = compute_miou(model, loader_val, n_cls, dev, model_type)\n",
    "            history.append({'epoch': epoch+1, 'loss': loss, 'miou': miou, 'pix_acc': pix_acc})\n",
    "            print(f'  Epoch {epoch+1:3d}/{epochs}: Loss={loss:.4f}, mIoU={miou:.4f}, PixAcc={pix_acc:.4f}')\n",
    "    # ìµœì¢… í‰ê°€\n",
    "    miou, iou_cls, pix_acc = compute_miou(model, loader_val, n_cls, dev, model_type)\n",
    "    print(f'\\nğŸ“Š {name} ìµœì¢…: mIoU={miou:.4f}, PixelAcc={pix_acc:.4f}')\n",
    "    return model, miou, iou_cls, pix_acc, history\n",
    "\n",
    "print('âœ… í•™ìŠµ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ')\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš©\n",
    "all_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-1. Model 1: DeepLabV3+ (ResNet-50 backbone)\n",
    "# ============================================================\n",
    "# Pretrained on COCO â†’ fine-tune í—¤ë“œë¥¼ ìš°ë¦¬ í´ë˜ìŠ¤ ìˆ˜ì— ë§ê²Œ êµì²´\n",
    "deeplab = deeplabv3_resnet50(weights='COCO_WITH_VOC_LABELS_V1')\n",
    "deeplab.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "deeplab.aux_classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "deeplab, dl_miou, dl_iou, dl_pacc, dl_hist = train_model(\n",
    "    deeplab, 'DeepLabV3+', train_loader, val_loader, num_classes, device\n",
    ")\n",
    "all_results['DeepLabV3+'] = {\n",
    "    'model': deeplab, 'miou': dl_miou, 'iou_per_class': dl_iou,\n",
    "    'pixel_acc': dl_pacc, 'history': dl_hist, 'type': 'standard'\n",
    "}\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ IoU\n",
    "print('\\nğŸ“‹ DeepLabV3+ í´ë˜ìŠ¤ë³„ IoU:')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'  {name:>15s}: {dl_iou[i]:.4f}')\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "gc.collect()\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-2. Model 2: SegFormer-B0 (HuggingFace Transformers)\n",
    "# ============================================================\n",
    "# Pretrained on ADE20K â†’ fine-tune for our classes\n",
    "segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    'nvidia/segformer-b0-finetuned-ade-512-512',\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "segformer, sf_miou, sf_iou, sf_pacc, sf_hist = train_model(\n",
    "    segformer, 'SegFormer-B0', train_loader, val_loader,\n",
    "    num_classes, device, model_type='segformer'\n",
    ")\n",
    "all_results['SegFormer-B0'] = {\n",
    "    'model': segformer, 'miou': sf_miou, 'iou_per_class': sf_iou,\n",
    "    'pixel_acc': sf_pacc, 'history': sf_hist, 'type': 'segformer'\n",
    "}\n",
    "\n",
    "print('\\nğŸ“‹ SegFormer-B0 í´ë˜ìŠ¤ë³„ IoU:')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'  {name:>15s}: {sf_iou[i]:.4f}')\n",
    "\n",
    "gc.collect()\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-3. BiSeNetV2 ì•„í‚¤í…ì²˜ ì •ì˜\n",
    "# ============================================================\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, ks=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, ks, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DetailBranch(nn.Module):\n",
    "    \"\"\"ê³µê°„ ì •ë³´ë¥¼ ë³´ì¡´í•˜ëŠ” ì–•ì€ ë¸Œëœì¹˜ (1/8 í•´ìƒë„)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.S1 = nn.Sequential(ConvBNReLU(3, 64, 3, 2, 1), ConvBNReLU(64, 64, 3, 1, 1))\n",
    "        self.S2 = nn.Sequential(ConvBNReLU(64, 64, 3, 2, 1), ConvBNReLU(64, 64, 3, 1, 1))\n",
    "        self.S3 = nn.Sequential(ConvBNReLU(64, 128, 3, 2, 1), ConvBNReLU(128, 128, 3, 1, 1))\n",
    "    def forward(self, x):\n",
    "        return self.S3(self.S2(self.S1(x)))\n",
    "\n",
    "class SemanticBranch(nn.Module):\n",
    "    \"\"\"ì˜ë¯¸ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ê¹Šì€ ë¸Œëœì¹˜ (ResNet-18 backbone, 1/32)\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights='IMAGENET1K_V1')\n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.compress = ConvBNReLU(512, 128, 1, 1, 0)\n",
    "    def forward(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return self.compress(x)\n",
    "\n",
    "class BiSeNetV2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.detail = DetailBranch()\n",
    "        self.semantic = SemanticBranch()\n",
    "        # ë‘ ë¸Œëœì¹˜ë¥¼ í•©ì¹˜ëŠ” Aggregation\n",
    "        self.agg_detail = ConvBNReLU(128, 128, 3, 1, 1)\n",
    "        self.agg_semantic = ConvBNReLU(128, 128, 3, 1, 1)\n",
    "        self.agg_out = ConvBNReLU(128, 128, 3, 1, 1)\n",
    "        self.head = nn.Sequential(ConvBNReLU(128, 64, 3, 1, 1), nn.Conv2d(64, num_classes, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[2:]\n",
    "        detail = self.detail(x)\n",
    "        semantic = self.semantic(x)\n",
    "        # Semanticì„ Detail í•´ìƒë„ì— ë§ì¶° ì—…ìƒ˜í”Œë§\n",
    "        sem_up = F.interpolate(semantic, size=detail.shape[2:], mode='bilinear', align_corners=False)\n",
    "        agg = self.agg_out(self.agg_detail(detail) + self.agg_semantic(sem_up))\n",
    "        out = self.head(agg)\n",
    "        return F.interpolate(out, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "print('âœ… BiSeNetV2 ì•„í‚¤í…ì²˜ ì •ì˜ ì™„ë£Œ')\n",
    "bisenet = BiSeNetV2(num_classes)\n",
    "total_params = sum(p.numel() for p in bisenet.parameters()) / 1e6\n",
    "print(f'ğŸ“Š BiSeNetV2 íŒŒë¼ë¯¸í„°: {total_params:.1f}M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-4. Model 3: BiSeNetV2 í•™ìŠµ\n",
    "# ============================================================\n",
    "bisenet, bs_miou, bs_iou, bs_pacc, bs_hist = train_model(\n",
    "    bisenet, 'BiSeNetV2', train_loader, val_loader, num_classes, device\n",
    ")\n",
    "all_results['BiSeNetV2'] = {\n",
    "    'model': bisenet, 'miou': bs_miou, 'iou_per_class': bs_iou,\n",
    "    'pixel_acc': bs_pacc, 'history': bs_hist, 'type': 'standard'\n",
    "}\n",
    "\n",
    "print('\\nğŸ“‹ BiSeNetV2 í´ë˜ìŠ¤ë³„ IoU:')\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f'  {name:>15s}: {bs_iou[i]:.4f}')\n",
    "\n",
    "gc.collect()\n",
    "if device.type == 'mps':\n",
    "    torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3-5. ëª¨ë¸ ë¹„êµ ìš”ì•½\n",
    "# ============================================================\n",
    "# ë¹„êµ í…Œì´ë¸”\n",
    "comp_data = []\n",
    "for name, res in all_results.items():\n",
    "    comp_data.append({'Model': name, 'mIoU': res['miou'], 'Pixel Acc': res['pixel_acc']})\n",
    "comp_df = pd.DataFrame(comp_data).sort_values('mIoU', ascending=False)\n",
    "print('ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:')\n",
    "display(comp_df)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "x = range(len(comp_df))\n",
    "axes[0].bar(x, comp_df['mIoU'], color=['#3498db', '#e74c3c', '#2ecc71'][:len(comp_df)])\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comp_df['Model'])\n",
    "axes[0].set_ylabel('mIoU')\n",
    "axes[0].set_title('ëª¨ë¸ë³„ mIoU ë¹„êµ')\n",
    "axes[0].set_ylim(0, 1)\n",
    "for i, v in enumerate(comp_df['mIoU']):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "axes[1].bar(x, comp_df['Pixel Acc'], color=['#3498db', '#e74c3c', '#2ecc71'][:len(comp_df)])\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(comp_df['Model'])\n",
    "axes[1].set_ylabel('Pixel Accuracy')\n",
    "axes[1].set_title('ëª¨ë¸ë³„ Pixel Accuracy ë¹„êµ')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(comp_df['Pixel Acc']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.suptitle('ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# í´ë˜ìŠ¤ë³„ IoU ë¹„êµ íˆíŠ¸ë§µ\n",
    "iou_df = pd.DataFrame({name: res['iou_per_class'] for name, res in all_results.items()}, index=class_names)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(iou_df, annot=True, fmt='.3f', cmap='YlGnBu', ax=ax, vmin=0, vmax=1)\n",
    "ax.set_title('ëª¨ë¸ë³„ í´ë˜ìŠ¤ IoU ë¹„êµ')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best model ì„ íƒ\n",
    "best_name = comp_df.iloc[0]['Model']\n",
    "best_model = all_results[best_name]['model']\n",
    "best_type = all_results[best_name]['type']\n",
    "print(f'\\nğŸ† Best Model: {best_name} (mIoU={all_results[best_name][\"miou\"]:.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ\n",
    "\n",
    "Best ëª¨ë¸ì˜ ì˜ˆì¸¡ ë§ˆìŠ¤í¬ì—ì„œ í´ë˜ìŠ¤ë³„ ë¹„ìœ¨ì„ ì¶”ì¶œí•˜ì—¬ **ê·œì¹™ ê¸°ë°˜ ì•ˆì „ ì ìˆ˜**ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.\n",
    "\n",
    "$$\\text{safety} = 0.35 \\times \\text{Road} + 0.15 \\times \\text{Lanemark} - 0.25 \\times \\text{Undrivable} - 0.25 \\times \\text{Movable}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4-1. ëª¨ë¸ ì˜ˆì¸¡ ê¸°ë°˜ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ\n",
    "# ============================================================\n",
    "# í´ë˜ìŠ¤ ì´ë¦„ â†’ ì¸ë±ìŠ¤ ë§¤í•‘ (class_names ê¸°ë°˜)\n",
    "name_to_idx = {n: i for i, n in enumerate(class_names)}\n",
    "\n",
    "def predict_mask(model, img_tensor, dev, model_type='standard'):\n",
    "    \"\"\"ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ì˜ˆì¸¡ ë§ˆìŠ¤í¬ ë°˜í™˜\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = img_tensor.unsqueeze(0).to(dev)\n",
    "        if model_type == 'segformer':\n",
    "            logits = model(pixel_values=x).logits\n",
    "            logits = F.interpolate(logits, size=img_tensor.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            out = model(x)\n",
    "            logits = out['out'] if isinstance(out, dict) else out\n",
    "    return logits.argmax(1).squeeze(0).cpu().numpy()\n",
    "\n",
    "def compute_safety_from_mask(pred_mask, name_to_idx, class_names):\n",
    "    \"\"\"ì˜ˆì¸¡ ë§ˆìŠ¤í¬ì—ì„œ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ\"\"\"\n",
    "    total = pred_mask.size\n",
    "    ratios = {}\n",
    "    for name in class_names:\n",
    "        idx = name_to_idx[name]\n",
    "        ratios[name] = np.sum(pred_mask == idx) / total\n",
    "\n",
    "    raw = (0.35 * ratios.get('Road', 0) +\n",
    "           0.15 * ratios.get('Lanemark', 0) -\n",
    "           0.25 * ratios.get('Undrivable', 0) -\n",
    "           0.25 * ratios.get('Movable', 0))\n",
    "    return raw, ratios\n",
    "\n",
    "# ì „ì²´ Val ì´ë¯¸ì§€ì— ëŒ€í•´ ì•ˆì „ ì ìˆ˜ ê³„ì‚°\n",
    "print(f'â³ {best_name}ìœ¼ë¡œ ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ ì¤‘...')\n",
    "safety_records = []\n",
    "\n",
    "for img_id in val_ids:\n",
    "    info = coco.loadImgs(img_id)[0]\n",
    "    img = np.array(Image.open(IMAGE_DIR / info['file_name']).convert('RGB'))\n",
    "    t = val_transform(image=img)\n",
    "    img_tensor = t['image'].float()\n",
    "    pred = predict_mask(best_model, img_tensor, device, best_type)\n",
    "    raw_score, ratios = compute_safety_from_mask(pred, name_to_idx, class_names)\n",
    "    safety_records.append({'image_id': img_id, 'file_name': info['file_name'],\n",
    "                           'safety_raw': raw_score, **ratios})\n",
    "\n",
    "safety_df = pd.DataFrame(safety_records)\n",
    "\n",
    "# MinMax ì •ê·œí™” â†’ 0~100\n",
    "s_min, s_max = safety_df['safety_raw'].min(), safety_df['safety_raw'].max()\n",
    "safety_df['safety_score'] = ((safety_df['safety_raw'] - s_min) / (s_max - s_min + 1e-10) * 100).round(1)\n",
    "safety_df['grade'] = safety_df['safety_score'].apply(\n",
    "    lambda s: 'Safe' if s >= 66 else ('Caution' if s >= 33 else 'Dangerous')\n",
    ")\n",
    "\n",
    "print(f'âœ… ì•ˆì „ ì ìˆ˜ ì‚°ì¶œ ì™„ë£Œ ({len(safety_df)}ì¥)')\n",
    "print(f'\\nğŸ“Š ì•ˆì „ ì ìˆ˜ í†µê³„:')\n",
    "print(safety_df['safety_score'].describe().round(1))\n",
    "print(f'\\nğŸ“Š ë“±ê¸‰ ë¶„í¬:')\n",
    "print(safety_df['grade'].value_counts())\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].hist(safety_df['safety_score'], bins=15, color='steelblue', edgecolor='white')\n",
    "axes[0].axvline(33, color='red', linestyle='--')\n",
    "axes[0].axvline(66, color='green', linestyle='--')\n",
    "axes[0].set_title('ì•ˆì „ ì ìˆ˜ ë¶„í¬')\n",
    "axes[0].set_xlabel('ì ìˆ˜')\n",
    "\n",
    "grade_counts = safety_df['grade'].value_counts()\n",
    "colors_pie = {'Safe': '#2ecc71', 'Caution': '#f39c12', 'Dangerous': '#e74c3c'}\n",
    "axes[1].pie(grade_counts.values, labels=grade_counts.index,\n",
    "            autopct='%1.1f%%', colors=[colors_pie.get(g, 'gray') for g in grade_counts.index])\n",
    "axes[1].set_title('ë“±ê¸‰ ë¶„í¬')\n",
    "\n",
    "axes[2].plot(range(len(safety_df)), safety_df['safety_score'].values, color='steelblue', alpha=0.7)\n",
    "axes[2].fill_between(range(len(safety_df)), 0, 33, alpha=0.1, color='red')\n",
    "axes[2].fill_between(range(len(safety_df)), 33, 66, alpha=0.1, color='orange')\n",
    "axes[2].fill_between(range(len(safety_df)), 66, 100, alpha=0.1, color='green')\n",
    "axes[2].set_title('í”„ë ˆì„ë³„ ì•ˆì „ ì ìˆ˜')\n",
    "axes[2].set_xlabel('í”„ë ˆì„')\n",
    "axes[2].set_ylabel('ì ìˆ˜')\n",
    "\n",
    "plt.suptitle(f'ì•ˆì „ ì ìˆ˜ ë¶„ì„ ({best_name})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Safe / Dangerous ëŒ€í‘œ í”„ë ˆì„ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "for i, (_, row) in enumerate(safety_df.nlargest(3, 'safety_score').iterrows()):\n",
    "    fuse_p = IMAGE_DIR / (row['file_name'] + '___fuse.png')\n",
    "    orig_p = IMAGE_DIR / row['file_name']\n",
    "    p = fuse_p if fuse_p.exists() else orig_p\n",
    "    if p.exists():\n",
    "        axes[0, i].imshow(Image.open(p))\n",
    "    axes[0, i].set_title(f'Safe: {row[\"safety_score\"]}ì ', color='green', fontweight='bold')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, (_, row) in enumerate(safety_df.nsmallest(3, 'safety_score').iterrows()):\n",
    "    fuse_p = IMAGE_DIR / (row['file_name'] + '___fuse.png')\n",
    "    orig_p = IMAGE_DIR / row['file_name']\n",
    "    p = fuse_p if fuse_p.exists() else orig_p\n",
    "    if p.exists():\n",
    "        axes[1, i].imshow(Image.open(p))\n",
    "    axes[1, i].set_title(f'Dangerous: {row[\"safety_score\"]}ì ', color='red', fontweight='bold')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('ì•ˆì „ ë“±ê¸‰ ë¹„êµ (ìƒìœ„ 3 vs í•˜ìœ„ 3)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. GradCAM ì‹œê°í™”\n",
    "\n",
    "**GradCAM** (Gradient-weighted Class Activation Mapping)ì„ í†µí•´ ê° ëª¨ë¸ì´ íŠ¹ì • í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ë•Œ ì´ë¯¸ì§€ì˜ **ì–´ëŠ ì˜ì—­ì— ì£¼ëª©**í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.\n",
    "- ê° ëª¨ë¸ì˜ ë§ˆì§€ë§‰ íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´ì— GradCAM ì ìš©\n",
    "- Road, Movable, Undrivable ë“± ì•ˆì „ì— í•µì‹¬ì ì¸ í´ë˜ìŠ¤ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë¶„ì„\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5-1. GradCAM ì‹œê°í™”\n",
    "# ============================================================\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import SemanticSegmentationTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# ---- ëª¨ë¸ë³„ íƒ€ê²Ÿ ë ˆì´ì–´ ì„¤ì • ----\n",
    "def get_target_layer(model, model_name):\n",
    "    \"\"\"ê° ëª¨ë¸ì˜ GradCAM íƒ€ê²Ÿ ë ˆì´ì–´ ë°˜í™˜\"\"\"\n",
    "    if 'DeepLab' in model_name:\n",
    "        return [model.backbone.layer4[-1]]\n",
    "    elif 'SegFormer' in model_name:\n",
    "        # SegFormer encoderì˜ ë§ˆì§€ë§‰ ë¸”ë¡\n",
    "        return [model.segformer.encoder.block[-1][-1].output.dense]\n",
    "    elif 'BiSeNet' in model_name:\n",
    "        return [model.semantic.layer4[-1]]\n",
    "    return []\n",
    "\n",
    "# ---- GradCAMìš© ëª¨ë¸ ë˜í¼ (SegFormerìš©) ----\n",
    "class SegformerWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        out = self.model(pixel_values=x)\n",
    "        logits = out.logits\n",
    "        return F.interpolate(logits, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "# ---- ì‹œê°í™” í•¨ìˆ˜ ----\n",
    "def denormalize(img_tensor):\n",
    "    \"\"\"ImageNet ì •ê·œí™” ì—­ë³€í™˜ â†’ [0,1] numpy\"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img = img * std + mean\n",
    "    return np.clip(img, 0, 1).astype(np.float32)\n",
    "\n",
    "# ì‹œê°í™” ëŒ€ìƒ í´ë˜ìŠ¤ (ì•ˆì „ì— í•µì‹¬ì ì¸ í´ë˜ìŠ¤)\n",
    "target_class_names = ['Road', 'Movable', 'Undrivable']\n",
    "target_class_ids = [name_to_idx.get(n, 0) for n in target_class_names if n in name_to_idx]\n",
    "target_class_names = [n for n in target_class_names if n in name_to_idx]\n",
    "\n",
    "# ìƒ˜í”Œ ì´ë¯¸ì§€ 3ì¥ ì¤€ë¹„\n",
    "sample_val_ids = val_ids[:3]\n",
    "sample_tensors = []\n",
    "sample_rgb = []\n",
    "for img_id in sample_val_ids:\n",
    "    info = coco.loadImgs(img_id)[0]\n",
    "    img = np.array(Image.open(IMAGE_DIR / info['file_name']).convert('RGB'))\n",
    "    t = val_transform(image=img)\n",
    "    sample_tensors.append(t['image'].float())\n",
    "    sample_rgb.append(denormalize(t['image'].float()))\n",
    "\n",
    "# ---- ëª¨ë¸ë³„ GradCAM ì‹œê°í™” ----\n",
    "for model_name, res in all_results.items():\n",
    "    mdl = res['model']\n",
    "    mtype = res['type']\n",
    "\n",
    "    try:\n",
    "        if mtype == 'segformer':\n",
    "            wrapped = SegformerWrapper(mdl)\n",
    "            target_layers = [mdl.segformer.encoder.block[-1][-1].output.dense]\n",
    "            cam_model = wrapped\n",
    "        else:\n",
    "            target_layers = get_target_layer(mdl, model_name)\n",
    "            cam_model = mdl\n",
    "\n",
    "        cam_model.eval()\n",
    "        cam = GradCAM(model=cam_model, target_layers=target_layers)\n",
    "\n",
    "        n_classes_show = len(target_class_names)\n",
    "        n_samples = len(sample_tensors)\n",
    "        fig, axes = plt.subplots(n_samples, n_classes_show + 1, figsize=(5 * (n_classes_show + 1), 5 * n_samples))\n",
    "        if n_samples == 1:\n",
    "            axes = axes[np.newaxis, :]\n",
    "\n",
    "        for si in range(n_samples):\n",
    "            # ì›ë³¸ ì´ë¯¸ì§€\n",
    "            axes[si, 0].imshow(sample_rgb[si])\n",
    "            axes[si, 0].set_title('ì›ë³¸', fontsize=10)\n",
    "            axes[si, 0].axis('off')\n",
    "\n",
    "            input_tensor = sample_tensors[si].unsqueeze(0)\n",
    "\n",
    "            # ì˜ˆì¸¡ ë§ˆìŠ¤í¬ ì–»ê¸°\n",
    "            with torch.no_grad():\n",
    "                if mtype == 'segformer':\n",
    "                    pred = cam_model(input_tensor.to(device)).argmax(1).squeeze(0).cpu().numpy()\n",
    "                else:\n",
    "                    out = mdl(input_tensor.to(device))\n",
    "                    logits = out['out'] if isinstance(out, dict) else out\n",
    "                    pred = logits.argmax(1).squeeze(0).cpu().numpy()\n",
    "\n",
    "            for ci, (cls_name, cls_id) in enumerate(zip(target_class_names, target_class_ids)):\n",
    "                # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë§ˆìŠ¤í¬ ì˜ì—­\n",
    "                cls_mask = (pred == cls_id).astype(np.float32)\n",
    "                if cls_mask.sum() < 10:\n",
    "                    axes[si, ci + 1].imshow(sample_rgb[si])\n",
    "                    axes[si, ci + 1].set_title(f'{cls_name}\\n(ë¯¸ê²€ì¶œ)', fontsize=10)\n",
    "                    axes[si, ci + 1].axis('off')\n",
    "                    continue\n",
    "\n",
    "                targets = [SemanticSegmentationTarget(cls_id, torch.from_numpy(cls_mask))]\n",
    "                grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n",
    "                cam_image = show_cam_on_image(sample_rgb[si], grayscale_cam, use_rgb=True)\n",
    "\n",
    "                axes[si, ci + 1].imshow(cam_image)\n",
    "                axes[si, ci + 1].set_title(f'{cls_name}', fontsize=10)\n",
    "                axes[si, ci + 1].axis('off')\n",
    "\n",
    "        plt.suptitle(f'GradCAM: {model_name}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f'âœ… {model_name} GradCAM ì™„ë£Œ')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'âš ï¸ {model_name} GradCAM ì‹¤íŒ¨: {e}')\n",
    "\n",
    "print('\\nğŸ“Œ GradCAM í•´ì„:')\n",
    "print('  - ë¹¨ê°„ ì˜ì—­: í•´ë‹¹ í´ë˜ìŠ¤ ì˜ˆì¸¡ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹œ ë¶€ë¶„')\n",
    "print('  - Road: ë„ë¡œ í‘œë©´ ìì²´ì— ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸')\n",
    "print('  - Movable: ì°¨ëŸ‰/ë³´í–‰ì ì˜ì—­ì— ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸')\n",
    "print('  - Undrivable: ì¸ë„/ê±´ë¬¼ ë“± ë¹„ì£¼í–‰ ì˜ì—­ì— ì§‘ì¤‘í•˜ëŠ”ì§€ í™•ì¸')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. ê²°ë¡ \n",
    "\n",
    "## 6-1. í”„ë¡œì íŠ¸ ìš”ì•½\n",
    "- **ë°ì´í„°**: Motorcycle Night Ride ë°ì´í„°ì…‹ (~200ì¥, 6í´ë˜ìŠ¤ ì„¸ë§Œí‹± ì„¸ê·¸ë©˜í…Œì´ì…˜)\n",
    "- **ëª¨ë¸**: DeepLabV3+, SegFormer-B0, BiSeNetV2 ì„¸ ê°€ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ fine-tuning\n",
    "- **ì•ˆì „ ì ìˆ˜**: ëª¨ë¸ ì˜ˆì¸¡ ë§ˆìŠ¤í¬ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ ê¸°ë°˜ ê·œì¹™ì  ì ìˆ˜ ì‚°ì¶œ\n",
    "- **XAI**: GradCAMìœ¼ë¡œ ëª¨ë¸ì˜ í´ë˜ìŠ¤ë³„ ì£¼ëª© ì˜ì—­ ì‹œê°í™”\n",
    "\n",
    "## 6-2. ì£¼ìš” ë°œê²¬\n",
    "1. **DeepLabV3+**: Atrous convolution ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ë„ë¡œ ì¥ë©´ì„ í¬ì°©. COCO pretrainedì˜ ì´ì \n",
    "2. **SegFormer-B0**: Transformer ê¸°ë°˜ìœ¼ë¡œ ì „ì—­ ë¬¸ë§¥(global context) í•™ìŠµì— ê°•ì . ê²½ëŸ‰ êµ¬ì¡°\n",
    "3. **BiSeNetV2**: Detail + Semantic ì´ì¤‘ ê²½ë¡œë¡œ ê³µê°„ ì •ë³´ì™€ ì˜ë¯¸ ì •ë³´ë¥¼ ë™ì‹œì— í•™ìŠµ\n",
    "4. **GradCAM ë¶„ì„**: ê° ëª¨ë¸ì´ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì‹¤ì œ ì˜ì—­ì— ì§‘ì¤‘í•˜ëŠ”ì§€ ì •ì„±ì ìœ¼ë¡œ í™•ì¸\n",
    "\n",
    "## 6-3. Metric ê·¼ê±°\n",
    "- **mIoU**: ì„¸ê·¸ë©˜í…Œì´ì…˜ì˜ í‘œì¤€ metric. í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡-GT ê²¹ì¹¨ ë¹„ìœ¨ì˜ í‰ê· \n",
    "- **Pixel Accuracy**: ì „ì²´ í”½ì…€ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ëœ ë¹„ìœ¨\n",
    "- **Safety Score**: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê°€ì¤‘í•© â†’ ì •ëŸ‰ì  ì•ˆì „ í‰ê°€\n",
    "\n",
    "## 6-4. í•œê³„ ë° í–¥í›„ ê³¼ì œ\n",
    "- **ë°ì´í„° ìˆ˜ ë¶€ì¡± (~200ì¥)**: ë” í° ë°ì´í„°ì…‹(BDD100K, Cityscapes ë“±)ê³¼ ê²°í•© ì‹œ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŒ€\n",
    "- **ì•¼ê°„ ì „ìš©**: ì£¼ê°„/ì•…ì²œí›„ ì¼ë°˜í™”ë¥¼ ìœ„í•´ ë‹¤ì–‘í•œ í™˜ê²½ ë°ì´í„° í•„ìš”\n",
    "- **ë‹¨ì¼ í”„ë ˆì„ ë¶„ì„**: ì˜ìƒ ì‹œê³„ì—´(temporal) ì •ë³´ í™œìš© ì‹œ ë” ì •êµí•œ ì•ˆì „ í‰ê°€ ê°€ëŠ¥\n",
    "- **GradCAM í•œê³„**: ì„¸ê·¸ë©˜í…Œì´ì…˜ì—ì„œëŠ” CAMì´ ëŒ€ëµì ì¸ ì˜ì—­ë§Œ ë³´ì—¬ì¤Œ â†’ Attention Map ì§ì ‘ ì‹œê°í™”ë„ ë³´ì™„ ê°€ëŠ¥\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
